# Text Processing Pipeline

This project implements a configurable, recursive text processing pipeline capable of handling various text analysis tasks. The pipeline is designed to be highly flexible and extensible, allowing for multiple stages of preprocessing, analysis, and post-processing, with dynamic function calls and parameter handling.

## Features

- **Configurable Stages**: The pipeline is divided into three main stages: Preprocessing, Analysis, and Post-Processing. Each stage can be configured independently using a JSON file.
- **Recursive Structure**: Supports nested configurations for handling multiple component extractions, such as multiple pipelines within a single configuration.
- **Dynamic Function Calls**: Functions are dynamically called based on their names specified in the configuration file. This allows for easy integration of functions defined in additional Python files.
- **Parameter Handling**: Each function can accept parameters specified in the configuration file, making the pipeline adaptable to various processing needs.
- **Metrics and Validation**: The pipeline includes metrics to determine if the results meet the desired standards. If the results do not meet the specified thresholds, the pipeline will re-process the data up to a maximum number of attempts.
- **Logging**: Comprehensive logging is implemented to track the process and capture errors. This includes a full log of all steps and errors, aiding in debugging and monitoring.

## Configuration

The pipeline's behavior is defined using a JSON configuration file. Hereâ€™s an example configuration:

```json
{
  "additional_files": ["additional_functions.py"],
  "pipelines": {
    "pipeline_1": {
      "preprocessing": {
        "tokenization": {
          "function": "tokenize",
          "params": {}
        },
        "normalization": {
          "function": "normalize",
          "params": {}
        },
        "stopword_removal": {
          "function": "remove_stopwords",
          "params": {}
        },
        "stemming": {
          "function": "stem",
          "params": {}
        },
        "lemmatization": {
          "function": "lemmatize",
          "params": {}
        }
      },
      "analysis": [
        {
          "task": "pos_tagging",
          "params": {},
          "metrics": {"accuracy_threshold": 0.95}
        },
        {
          "task": "ner",
          "params": {},
          "metrics": {"accuracy_threshold": 0.90}
        },
        {
          "task": "keyword_extraction",
          "params": {
            "method": "tfidf",
            "top_n": 10
          },
          "metrics": {"precision_threshold": 0.8}
        },
        {
          "task": "tfidf",
          "params": {}
        },
        {
          "task": "sentiment_analysis",
          "params": {},
          "metrics": {"sentiment_score_threshold": 0.75}
        }
      ],
      "postprocessing": {
        "filtering": {
          "function": "filter_results",
          "params": {}
        },
        "aggregation": {
          "function": "aggregate_results",
          "params": {}
        },
        "export": {
          "function": "export_results",
          "params": {
            "path": "./results.csv"
          }
        }
      }
    }
  }
}
```

## Usage

### Initialize and Run the Pipeline

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-repo/text-processing-pipeline.git
   cd text-processing-pipeline
   ```

2. **Install Dependencies**:
   Ensure you have the required dependencies installed. You can use `pip` to install them:
   ```bash
   pip install -r requirements.txt
   ```

3. **Create or Edit Configuration File**:
   Create a configuration file (e.g., `config.json`) as shown in the example above. Edit it to suit your requirements.

4. **Run the Pipeline**:
   Use the provided script to run the pipeline:
   ```python
   import json
   from pipeline import TextProcessingPipeline

   # Load configuration
   with open('config.json', 'r') as f:
       config = json.load(f)

   # Sample text
   text = "The quick brown fox jumps over the lazy dog."

   # Initialize and run pipeline
   pipeline = TextProcessingPipeline(config)
   pipeline.run(text)
   ```

## Logging and Error Handling

- **Logs**: The system logs all steps, including preprocessing, analysis, and post-processing steps, along with errors and warnings. Logs are written to `pipeline.log`.
- **Error Handling**: If a task fails to meet the specified metrics after a maximum number of attempts, an error message is logged.

## Adding New Functions

To add new functions, you can define them in additional Python files and include these files in the `additional_files` list in the configuration. The pipeline will dynamically load and use these functions based on the configuration.

---

This README provides a comprehensive overview of the text processing pipeline, including its features, configuration, usage, logging, and error handling. Feel free to adjust it according to your project's needs! ðŸ˜Š

Is there anything specific you'd like to adjust or add to this description?
